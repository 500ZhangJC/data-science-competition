{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import xlrd\n",
    "import json\n",
    "from collections import Counter\n",
    "#这一步将可以正常读取的文件筛选出来\n",
    "def dis_error_normal(paths):\n",
    "    error=[]\n",
    "    normal=[]\n",
    "    for i in tqdm(range(len(paths))):\n",
    "        name=paths[i]\n",
    "        try:\n",
    "            tmp=pd.read_csv(name)\n",
    "            normal.append(name)\n",
    "        except:\n",
    "            error.append(name)\n",
    "            continue\n",
    "    return error,normal\n",
    "#只筛选中文\n",
    "def clean_data(data_in):\n",
    "    # coding=utf-8\n",
    "    if data_in==\"\":\n",
    "        return \"\"\n",
    "    pchinese = re.compile('([\\u4e00-\\u9fa5]+)+?')\n",
    "    data_out = pchinese.findall(data_in)\n",
    "    if data_out:\n",
    "        data_out = ''.join(data_out)\n",
    "    if data_out == []:\n",
    "        return \"\"\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(NROWS,topk):\n",
    "    #\n",
    "    print(\"读取训练数据－－－－－－－－－－－－－\")\n",
    "    # 获取每个文件的内容，和文件标题一起进行保存\n",
    "    save_data = 'fastTextData/'\n",
    "    if not os.path.exists(save_data):\n",
    "        os.mkdir(save_data)\n",
    "    train_val_df = {\"text\":[],\"label\":[]}\n",
    "    #\n",
    "    for per_path in tqdm(train_paths):\n",
    "        # 结巴分词\n",
    "        text_title = per_path.split('/')[-1]\n",
    "        text_title = clean_data(text_title)\n",
    "        flag = filename_label[per_path]\n",
    "        text_content=[]\n",
    "        if per_path not in train_error_clean:  # html文件直接跳过\n",
    "            if (per_path.split('.')[-1] == 'csv') or (per_path in train_normal_clean):\n",
    "                tmp_csv = pd.read_csv(per_path).reset_index(drop=True)  # 这里必须reset_index,不然会有bug\n",
    "                text_content = clean_data(''.join(tmp_csv[:NROWS].values.reshape(-1, ).astype(str).tolist()))\n",
    "            else:\n",
    "                bk = xlrd.open_workbook(per_path)\n",
    "                sheet_cnt = len(bk.sheet_names())\n",
    "                if sheet_cnt > 1:\n",
    "                    sh = bk.sheet_by_name(bk.sheet_names()[1])\n",
    "                else:\n",
    "                    sh = bk.sheet_by_name(bk.sheet_names()[0])\n",
    "                #\n",
    "                nrows = sh.nrows\n",
    "                ncols = sh.ncols\n",
    "                tmp_xls = \"\"\n",
    "                if nrows > NROWS:\n",
    "                    nrows = NROWS\n",
    "                for i in range(1, nrows):\n",
    "                    tmp = [str(jj) for jj in sh.row_values(i)]\n",
    "                    row_data = ''.join(tmp)\n",
    "                    tmp_xls += row_data\n",
    "                #\n",
    "                text_content = clean_data(tmp_xls)\n",
    "            #这里是提取关键词\n",
    "            if len(text_content) > 0:  # 如果内容文件不为空\n",
    "                text_content_count = list(jieba.cut(text_content))  # 分词结果\n",
    "                counter = Counter(text_content_count)\n",
    "                #前topk高频词\n",
    "                text_content_count = \"\".join([ii[0] for ii in counter.most_common(topk)])\n",
    "            else:\n",
    "                text_content_count=\"\"\n",
    "        #\n",
    "        if len(text_content) == 0:\n",
    "            text_content=\"\"\n",
    "        text = text_title+text_content_count+text_content\n",
    "        #text = text_content\n",
    "        #\n",
    "        if len(text)==0:\n",
    "            text=\"工业\"\n",
    "        assert len(text) > 0\n",
    "        #\n",
    "        train_val_df[\"text\"].append(text)\n",
    "        train_val_df[\"label\"].append(int(flag))\n",
    "    #\n",
    "    train_val_df=pd.DataFrame(train_val_df)\n",
    "    train_val_df.to_csv(\"./dataset/train_val_df_127_\"+str(NROWS)+\".csv\",index=False)\n",
    "\n",
    "def get_test_data(NROWS,topk):\n",
    "    print(\"读取测试数据－－－－－－－－－－－－－\")\n",
    "    test_data_df={\"name\":[],\"text\":[]}\n",
    "    sub_sample=pd.read_csv(\"submit_example_test2.csv\")\n",
    "    sub_filenames=sub_sample['filename'].values\n",
    "    for per_path in tqdm(sub_filenames):\n",
    "        # 结巴分词\n",
    "        text_title = per_path.split('/')[-1]\n",
    "        text_title = clean_data(text_title)#\n",
    "        #\n",
    "        text_content = []\n",
    "        if per_path not in test_error_clean:  #html文件直接跳过\n",
    "            if (per_path.split('.')[-1] == 'csv') or (per_path in test_normal_clean):\n",
    "                tmp_csv = pd.read_csv(per_path).reset_index(drop=True)  # 这里必须reset_index,不然会有bug\n",
    "                text_content = clean_data(''.join(tmp_csv[:NROWS].values.reshape(-1, ).astype(str).tolist()))\n",
    "            else:\n",
    "                bk=xlrd.open_workbook(per_path)\n",
    "                sheet_cnt= len(bk.sheet_names())\n",
    "                if sheet_cnt>1:\n",
    "                    sh = bk.sheet_by_name(bk.sheet_names()[1])\n",
    "                else:\n",
    "                    sh= bk.sheet_by_name(bk.sheet_names()[0])\n",
    "                #\n",
    "                nrows = sh.nrows\n",
    "                ncols = sh.ncols\n",
    "                tmp_xls = \"\"\n",
    "                if nrows>NROWS:\n",
    "                    nrows=NROWS\n",
    "                for i in range(1, nrows):\n",
    "                    tmp=[str(jj) for jj in sh.row_values(i)]\n",
    "                    row_data = ''.join(tmp)\n",
    "                    tmp_xls+=row_data\n",
    "                #\n",
    "                text_content = clean_data(tmp_xls)\n",
    "            #这里是提取关键词\n",
    "            if len(text_content) > 0:  # 如果内容文件不为空\n",
    "                text_content_count = list(jieba.cut(text_content))  # 分词结果\n",
    "                counter = Counter(text_content_count)\n",
    "                #前topk高频词\n",
    "                text_content_count = \"\".join([ii[0] for ii in counter.most_common(topk)])\n",
    "            else:\n",
    "                text_content_count=\"\"\n",
    "        #\n",
    "        if len(text_content) == 0:\n",
    "            text_content=\"\"\n",
    "        text = text_title+text_content_count+text_content\n",
    "        #text = text_content\n",
    "        if len(text)==0:\n",
    "            text=\"工业\"\n",
    "        assert len(text) > 0\n",
    "        #\n",
    "        test_data_df[\"name\"].append(per_path)\n",
    "        test_data_df[\"text\"].append(text)\n",
    "    test_data_df = pd.DataFrame(test_data_df)\n",
    "    test_data_df.to_csv(\"./dataset/test2_df_127_\"+str(NROWS)+\".csv\", index=False)\n",
    "#a榜数据用作辅助\n",
    "def get_test_data_1(NROWS,topk):\n",
    "    print(\"读取测试数据－－－－－－－－－－－－－\")\n",
    "    test_data_df={\"name\":[],\"text\":[]}\n",
    "    sub_sample=pd.read_csv(\"submit_example_test1.csv\")\n",
    "    sub_filenames=sub_sample['filename'].values\n",
    "    for per_path in tqdm(sub_filenames):\n",
    "        # 结巴分词\n",
    "        text_title = per_path.split('/')[-1]\n",
    "        text_title = clean_data(text_title)#\n",
    "        #\n",
    "        text_content = []\n",
    "        if per_path not in test_error_clean_1:  #html文件直接跳过\n",
    "            if (per_path.split('.')[-1] == 'csv') or (per_path in test_normal_clean_1):\n",
    "                tmp_csv = pd.read_csv(per_path).reset_index(drop=True)  # 这里必须reset_index,不然会有bug\n",
    "                text_content = clean_data(''.join(tmp_csv[:NROWS].values.reshape(-1, ).astype(str).tolist()))\n",
    "            else:\n",
    "                bk=xlrd.open_workbook(per_path)\n",
    "                sheet_cnt= len(bk.sheet_names())\n",
    "                if sheet_cnt>1:\n",
    "                    sh = bk.sheet_by_name(bk.sheet_names()[1])\n",
    "                else:\n",
    "                    sh= bk.sheet_by_name(bk.sheet_names()[0])\n",
    "                #\n",
    "                nrows = sh.nrows\n",
    "                ncols = sh.ncols\n",
    "                tmp_xls = \"\"\n",
    "                if nrows>NROWS:\n",
    "                    nrows=NROWS\n",
    "                for i in range(1, nrows):\n",
    "                    tmp=[str(jj) for jj in sh.row_values(i)]\n",
    "                    row_data = ''.join(tmp)\n",
    "                    tmp_xls+=row_data\n",
    "                #\n",
    "                text_content = clean_data(tmp_xls)\n",
    "            #这里是提取关键词\n",
    "            if len(text_content) > 0:  # 如果内容文件不为空\n",
    "                text_content_count = list(jieba.cut(text_content))  # 分词结果\n",
    "                counter = Counter(text_content_count)\n",
    "                #前topk高频词\n",
    "                text_content_count = \"\".join([ii[0] for ii in counter.most_common(topk)])\n",
    "            else:\n",
    "                text_content_count=\"\"\n",
    "        #\n",
    "        if len(text_content) == 0:\n",
    "            text_content=\"\"\n",
    "        text = text_title+text_content_count+text_content\n",
    "        #text = text_content\n",
    "        if len(text)==0:\n",
    "            text=\"工业\"\n",
    "        assert len(text) > 0\n",
    "        #\n",
    "        test_data_df[\"name\"].append(per_path)\n",
    "        test_data_df[\"text\"].append(text)\n",
    "    test_data_df = pd.DataFrame(test_data_df)\n",
    "    test_data_df.to_csv(\"./dataset/test1_df_127_\"+str(NROWS)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '文化休闲', 1: '医疗卫生', 2: '经济管理', 3: '教育科技', 4: '城乡建设', 5: '工业', 6: '民政社区', 7: '交通运输', 8: '生态环境', 9: '政法监察', 10: '农业畜牧业', 11: '文秘行政', 12: '劳动人事', 13: '资源能源', 14: '信息产业', 15: '旅游服务', 16: '商业贸易', 17: '气象水文测绘地震地理', 18: '财税金融', 19: '外交外事'}\n"
     ]
    }
   ],
   "source": [
    "with open('./tmp/train_error_clean.json','r',encoding='UTF-8') as f:\n",
    "        train_error_clean=json.load(f)\n",
    "with open('./tmp/train_normal_clean.json','r',encoding='UTF-8') as f:\n",
    "    train_normal_clean=json.load(f)\n",
    "with open('./tmp/test_error_clean.json','r',encoding='UTF-8') as f:\n",
    "    test_error_clean=json.load(f)\n",
    "with open('./tmp/test_normal_clean.json','r',encoding='UTF-8') as f:\n",
    "    test_normal_clean=json.load(f)\n",
    "with open('./tmp/test_error_clean_1.json','r',encoding='UTF-8') as f:\n",
    "    test_error_clean_1=json.load(f)\n",
    "with open('./tmp/test_normal_clean_1.json','r',encoding='UTF-8') as f:\n",
    "    test_normal_clean_1=json.load(f)\n",
    "#----------编码－－－－－－－－－－－－\n",
    "answer_train = pd.read_csv('answer_train.csv')\n",
    "encode_dic = {}\n",
    "cate = answer_train.label.unique()\n",
    "for i in range(len(cate)):\n",
    "    encode_dic[cate[i]] = i\n",
    "answer_train['label'] = answer_train['label'].map(encode_dic)\n",
    "filename_label = {}\n",
    "for index, filename, label in answer_train.itertuples():\n",
    "    filename_label[filename] = label\n",
    "#---------解码---------\n",
    "decode_dic = {}\n",
    "for key in encode_dic.keys():\n",
    "    decode_dic[encode_dic[key]] = key\n",
    "print(decode_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 11/8000 [00:00<01:15, 106.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取测试数据－－－－－－－－－－－－－\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████                                                                 | 1255/8000 [00:10<01:05, 103.42it/s]C:\\Users\\Administrator\\AppData\\Local\\Continuum\\anaconda3\\envs\\huawei\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3254: DtypeWarning: Columns (14,23,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [01:17<00:00, 103.35it/s]\n"
     ]
    }
   ],
   "source": [
    "test_paths_1 = glob.glob('test1/*')  # 获取测试集所以文件绝对路径\n",
    "test_paths_1 = [sample.replace('\\\\', '/') for sample in test_paths_1]\n",
    "# test_error,test_normal=dis_error_normal(test_paths)\n",
    "get_test_data_1(100,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                | 4/25439 [00:00<12:17, 34.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取测试数据－－－－－－－－－－－－－\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 25439/25439 [06:24<00:00, 66.23it/s]\n"
     ]
    }
   ],
   "source": [
    "test_paths = glob.glob('test2/*')  # 获取测试集所以文件绝对路径\n",
    "test_paths = [sample.replace('\\\\', '/') for sample in test_paths]\n",
    "# test_error,test_normal=dis_error_normal(test_paths)\n",
    "get_test_data(100,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                              | 1/59985 [00:00<1:58:31,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取训练数据－－－－－－－－－－－－－\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 59985/59985 [11:25<00:00, 87.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# 获取每个文件的内容,和文件标题一起进行保存\n",
    "train_paths = glob.glob('train/*')  # 获取训练集所以文件绝对路径\n",
    "train_paths = [sample.replace('\\\\', '/') for sample in train_paths]\n",
    "get_train_data(100,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dis_error_normal(paths):\n",
    "#     error=[]\n",
    "#     normal=[]\n",
    "#     for i in tqdm(range(len(paths))):\n",
    "#         name=paths[i]\n",
    "#         if name.split('.')[-1] == 'csv':#以csv结尾的文件利用pandas读取\n",
    "#             try:\n",
    "#                 tmp=pd.read_csv(name)\n",
    "#                 normal.append(name)\n",
    "#             except:\n",
    "#                 error.append(name)\n",
    "#                 continue\n",
    "#         else:\n",
    "#             try:\n",
    "#                 tmp=xlrd.open_workbook(name)\n",
    "#                 normal.append(name)\n",
    "#             except:\n",
    "#                 error.append(name)\n",
    "#                 continue\n",
    "#     return error,normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 8000/8000 [00:40<00:00, 199.28it/s]\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Continuum\\anaconda3\\envs\\huawei\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (14,23,24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1221, 19)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_paths = glob.glob('test1/*')  # 获取测试集所有文件绝对路径\n",
    "# test_paths = [sample.replace('\\\\', '/') for sample in test_paths]\n",
    "# test_error,test_normal=dis_error_normal(test_paths)\n",
    "# test_error_clean=[]\n",
    "# test_normal_clean=[]\n",
    "# for name in test_error:\n",
    "#     try:\n",
    "#         tmp=pd.read_csv(name)\n",
    "#         test_normal_clean.append(name)\n",
    "#     except:\n",
    "#         test_error_clean.append(name)\n",
    "# #train_normal_clean:可以被pandas 打开的xls文件，train_error_clean错误文件\n",
    "# len(test_normal_clean),len(test_error_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('test_error_clean_1.json', 'w', encoding='UTF-8') as fp:\n",
    "#     fp.write(json.dumps(test_error_clean, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huawei",
   "language": "python",
   "name": "huawei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
